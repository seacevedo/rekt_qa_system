{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68c3fe1e-77e3-496e-bcd8-8eadb8ed8152",
   "metadata": {},
   "outputs": [
    {
     "ename": "WebDriverException",
     "evalue": "Message: Service /home/seacevedo/.cache/selenium/chromedriver/linux64/128.0.6613.137/chromedriver unexpectedly exited. Status code was: 127\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mWebDriverException\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 70\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m all_articles\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# Run the scraper\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m scraped_articles \u001b[38;5;241m=\u001b[39m \u001b[43mscrape_rekt_news\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 21\u001b[0m, in \u001b[0;36mscrape_rekt_news\u001b[0;34m(base_url, num_pages)\u001b[0m\n\u001b[1;32m     18\u001b[0m page \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Set up Selenium WebDriver (make sure you have the appropriate driver installed)\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m driver \u001b[38;5;241m=\u001b[39m \u001b[43mwebdriver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mChrome\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# or webdriver.Firefox(), etc.\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m page \u001b[38;5;241m<\u001b[39m num_pages:\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/rekt_qa_system-pBD0ZnT5/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py:45\u001b[0m, in \u001b[0;36mWebDriver.__init__\u001b[0;34m(self, options, service, keep_alive)\u001b[0m\n\u001b[1;32m     42\u001b[0m service \u001b[38;5;241m=\u001b[39m service \u001b[38;5;28;01mif\u001b[39;00m service \u001b[38;5;28;01melse\u001b[39;00m Service()\n\u001b[1;32m     43\u001b[0m options \u001b[38;5;241m=\u001b[39m options \u001b[38;5;28;01mif\u001b[39;00m options \u001b[38;5;28;01melse\u001b[39;00m Options()\n\u001b[0;32m---> 45\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbrowser_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDesiredCapabilities\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCHROME\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbrowserName\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvendor_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgoog\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mservice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mservice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_alive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/rekt_qa_system-pBD0ZnT5/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py:55\u001b[0m, in \u001b[0;36mChromiumDriver.__init__\u001b[0;34m(self, browser_name, vendor_prefix, options, service, keep_alive)\u001b[0m\n\u001b[1;32m     52\u001b[0m     options\u001b[38;5;241m.\u001b[39mbrowser_version \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mservice\u001b[38;5;241m.\u001b[39mpath \u001b[38;5;241m=\u001b[39m finder\u001b[38;5;241m.\u001b[39mget_driver_path()\n\u001b[0;32m---> 55\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mservice\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m executor \u001b[38;5;241m=\u001b[39m ChromiumRemoteConnection(\n\u001b[1;32m     58\u001b[0m     remote_server_addr\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mservice\u001b[38;5;241m.\u001b[39mservice_url,\n\u001b[1;32m     59\u001b[0m     browser_name\u001b[38;5;241m=\u001b[39mbrowser_name,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     62\u001b[0m     ignore_proxy\u001b[38;5;241m=\u001b[39moptions\u001b[38;5;241m.\u001b[39m_ignore_local_proxy,\n\u001b[1;32m     63\u001b[0m )\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/rekt_qa_system-pBD0ZnT5/lib/python3.10/site-packages/selenium/webdriver/common/service.py:102\u001b[0m, in \u001b[0;36mService.start\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    100\u001b[0m count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 102\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massert_process_still_running\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_connectable():\n\u001b[1;32m    104\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/rekt_qa_system-pBD0ZnT5/lib/python3.10/site-packages/selenium/webdriver/common/service.py:115\u001b[0m, in \u001b[0;36mService.assert_process_still_running\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    113\u001b[0m return_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess\u001b[38;5;241m.\u001b[39mpoll()\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_code:\n\u001b[0;32m--> 115\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m WebDriverException(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mService \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m unexpectedly exited. Status code was: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreturn_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mWebDriverException\u001b[0m: Message: Service /home/seacevedo/.cache/selenium/chromedriver/linux64/128.0.6613.137/chromedriver unexpectedly exited. Status code was: 127\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import uuid\n",
    "import hashlib\n",
    "import pandas as pd  \n",
    "\n",
    "def create_uuid_from_string(val):\n",
    "    hex_string = hashlib.md5(val.encode(\"UTF-8\")).hexdigest()\n",
    "    return str(uuid.UUID(hex=hex_string))[:8]\n",
    "\n",
    "def scrape_rekt_news(base_url='https://rekt.news/', num_pages=28):\n",
    "    all_articles = []\n",
    "    page = 0\n",
    "\n",
    "    # Set up Selenium WebDriver (make sure you have the appropriate driver installed)\n",
    "    driver = webdriver.Chrome()  # or webdriver.Firefox(), etc.\n",
    "\n",
    "    try:\n",
    "        while page < num_pages:\n",
    "            url = f'{base_url}?page={page}'\n",
    "            driver.get(url)\n",
    "\n",
    "            # Wait for the articles to load\n",
    "            try:\n",
    "                WebDriverWait(driver, 10).until(\n",
    "                    EC.presence_of_element_located((By.CLASS_NAME, \"post\"))\n",
    "                )\n",
    "            except TimeoutException:\n",
    "                print(f\"No more articles found on page {page}. Stopping.\")\n",
    "                break\n",
    "\n",
    "            # Parse the page source with BeautifulSoup\n",
    "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "            articles = soup.find_all('article', class_='post')\n",
    "\n",
    "            if not articles:\n",
    "                print(f\"No more articles found on page {page}. Stopping.\")\n",
    "                break\n",
    "\n",
    "            for article in articles:\n",
    "                title = article.header.h5.a.text\n",
    "                summary = article.section.p.text\n",
    "                date = article.header.span.time.text\n",
    "                tags = article.header.span.p.text.replace(\" - \", \",\")\n",
    "                #uuid = create_uuid_from_string(title)\n",
    "\n",
    "                all_articles.append({\n",
    "                    'title': title,\n",
    "                    'date': date,\n",
    "                    'tags': tags,\n",
    "                    'summary': summary,\n",
    "                    #'id': uuid\n",
    "                })\n",
    "\n",
    "            print(f\"Scraped page {page}\")\n",
    "            page += 1\n",
    "            time.sleep(2)  # Be polite and avoid overwhelming the server\n",
    "\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "    return all_articles\n",
    "\n",
    "# Run the scraper\n",
    "scraped_articles = scrape_rekt_news()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ba5bdd-5251-4da8-b4a9-0d9f277fa384",
   "metadata": {},
   "outputs": [],
   "source": [
    "scraped_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8dc65947-ac7e-4023-a1cb-ab9d517598e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "rekt_df = pd.json_normalize(scraped_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "47cd9ee2-79a9-49df-bf95-075a1b0ff63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rekt_df.to_csv('../datasets/rekt_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c53d7a0a-ecbe-463c-95fd-6a6a4f82e230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reached maximum scroll height of 1 px\n",
      "Title: Hacker steals $1.45 million from CUT token liquidity pool\n",
      "Date: 2024-09-10\n",
      "Summary: An attacker exploited a bug in the smart contract for a BSC-based token called CUT, draining a PancakeSwap liquidity pool of almost $1.45 million in the BSC-USD stablecoin.\n",
      "Tags: Hack or scam\n",
      "---\n",
      "Title: Indodax crypto exchange apparently hacked for at least $22 million\n",
      "Date: 2024-09-10\n",
      "Summary: The Indonesian Indodax cryptocurrency exchange suffered an exploit that allowed attackers to steal tokens from several of its hot wallets. The firm did not directly acknowledge the theft, instead posting an announcement that they had \"discovered a potential security issue\" and were \"conducting a complete maintenance to ensure the entire system is operating properly\". They reassured customers that their assets were \"100% safe\".Indodax's Instagram account also appeared to be compromised, promoting a suspicious \"giveaway\".\n",
      "Tags: Hack or scam\n",
      "---\n",
      "Total articles scraped: 2\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.remote.webdriver import WebDriver\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List\n",
    "from selenium.common.exceptions import WebDriverException\n",
    "\n",
    "\n",
    "def is_date_in_current_week(date: datetime) -> bool:\n",
    "    \"\"\"Checks if a given date is in the current week.\"\"\"\n",
    "    today = datetime.today()\n",
    "    start_of_week = today - timedelta(days=today.weekday())\n",
    "    end_of_week = start_of_week + timedelta(days=6)\n",
    "\n",
    "    return start_of_week <= date <= end_of_week\n",
    "\n",
    "def setup_driver() -> WebDriver:\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    try:\n",
    "        return webdriver.Remote(\"http://127.0.0.1:4444/wd/hub\", options=chrome_options)\n",
    "    except WebDriverException:\n",
    "        print(\"Unable to connect to driver\")\n",
    "\n",
    "def scroll_to_bottom(driver: WebDriver, max_scroll_height: int) -> None:\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    while True:\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(2)  # Wait for new content to load\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if max_scroll_height and new_height >= max_scroll_height:\n",
    "            print(f\"Reached maximum scroll height of {max_scroll_height} px\")\n",
    "            break\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "        \n",
    "def scrape_article_cards(driver: WebDriver) -> List[dict]:\n",
    "    articles = []\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    cards = soup.find_all('div', class_='timeline-description')\n",
    "    \n",
    "    for card in cards:\n",
    "        title = card.find('h2').text.strip()\n",
    "        date = card.find('span', class_='timestamp').text.strip()\n",
    "        summary = card.find('div', class_='timeline-body-text-wrapper').text.strip()\n",
    "        tags = card.find('div', class_='tag-list theme').text.replace('Theme tags: ', '').strip()\n",
    "\n",
    "        date_object = datetime.strptime(date, \"%B %d, %Y\")\n",
    "\n",
    "        if is_date_in_current_week(date_object) and 'Hack' in tags:\n",
    "\n",
    "            formatted_date_obj = date_object.strftime(\"%Y-%m-%d\")\n",
    "            \n",
    "            articles.append({\n",
    "                'title': title,\n",
    "                'date': formatted_date_obj,\n",
    "                'summary': summary,\n",
    "                'tags': tags\n",
    "            })\n",
    "\n",
    "\n",
    "    \n",
    "    return articles\n",
    "\n",
    "def main():\n",
    "    driver = setup_driver()\n",
    "    driver.get('https://www.web3isgoinggreat.com/')\n",
    "    max_scroll_height = 1\n",
    "    \n",
    "    try:\n",
    "        # Wait for the first card to load\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.CLASS_NAME, \"timeline-description\"))\n",
    "        )\n",
    "        \n",
    "        # Scroll to load all content\n",
    "        scroll_to_bottom(driver, max_scroll_height)\n",
    "        \n",
    "        # Scrape the loaded content\n",
    "        articles = scrape_article_cards(driver)\n",
    "\n",
    "        df = pd.json_normalize(articles)\n",
    "\n",
    "        df.to_csv('../datasets/web3isgoinggreat_dataset_1.csv')\n",
    "        \n",
    "        # Print the results\n",
    "        for article in articles:\n",
    "            print(f\"Title: {article['title']}\")\n",
    "            print(f\"Date: {article['date']}\")\n",
    "            print(f\"Summary: {article['summary']}\")\n",
    "            print(f\"Tags: {article['tags']}\")\n",
    "            print(\"---\")\n",
    "        \n",
    "        print(f\"Total articles scraped: {len(articles)}\")\n",
    "        \n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0faa4de9-152c-48b3-97e3-25053f5a2d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime.datetime"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
